{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 12377)\n",
      "(500, 12377)\n",
      "(2000,)\n",
      "(500, 10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import help\n",
    "\n",
    "X_class, X_family, y_class, y_family = help.loadDataset(5, feature_scailing=True)\n",
    "print(X_class.shape)\n",
    "print(X_family.shape)\n",
    "print(y_class.shape)\n",
    "print(y_family.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 12377)\n",
      "(400, 12377)\n",
      "(1600,)\n",
      "(400,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_class,y_class,test_size=0.2,random_state=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def keras_nn_for_class(X_train, X_test, Y_train, Y_test, dropout = 0.5, batch_size = 32):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Flatten, Dense, Lambda, Cropping2D, Dropout\n",
    "    from keras.layers.convolutional import Convolution2D\n",
    "    \n",
    "    input_node = X_train.shape[1]\n",
    "    \n",
    "    layer = []\n",
    "    batch_size = 16\n",
    "    \n",
    "    train_generator = generator(X_train, Y_train, batch_size=batch_size)\n",
    "    validation_generator = generator(X_test, Y_test, batch_size=batch_size)\n",
    "    \n",
    "    input_shape =(X_train.shape[1],)  # Trimmed image format\n",
    "    #print(\"input_shape:\", input_shape)\n",
    "        \n",
    "    model = Sequential()\n",
    "    # Preprocess incoming data, centered around zero with small standard deviation \n",
    "    model.add(Dense(input_node, activation='relu', input_shape=input_shape , kernel_initializer='normal'))\n",
    "    layer.append(input_node)\n",
    "    model.add(Dropout(dropout)) # for preventing overfit\n",
    "    layer.append(\"Dropout\")\n",
    "    input_node = input_node//2\n",
    "    \n",
    "    \n",
    "    while(input_node > 1):\n",
    "        model.add(Dense(input_node, activation='relu',  kernel_initializer='normal'))\n",
    "        layer.append(input_node)\n",
    "        model.add(Dropout(dropout)) # for preventing overfit\n",
    "        layer.append(\"Dropout\")\n",
    "        input_node = input_node//2\n",
    "        \n",
    "        \n",
    "    model.add(Dense(1, activation='sigmoid', kernel_initializer='normal'))\n",
    "    layer.append(1)\n",
    "    #print('layer:', layer)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    #########################################################################\n",
    "    # train a model\n",
    "\n",
    "    history_object = model.fit_generator(train_generator, steps_per_epoch=len(X_train)/batch_size,\n",
    "                        validation_data=validation_generator,\n",
    "                        validation_steps=len(X_test)/batch_size, epochs=10,verbose=0)\n",
    "    \n",
    "    #generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
    "    \n",
    "    loss, score = model.evaluate_generator(validation_generator, steps=len(X_test)/batch_size, max_queue_size=10, workers=1, use_multiprocessing=True)\n",
    "    return score\n",
    "\n",
    "def generator(X, y, batch_size=32):\n",
    "    from sklearn.utils import shuffle\n",
    "    \n",
    "    num_samples = len(X)\n",
    "    \n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_X = X[offset:offset+batch_size]\n",
    "            batch_y = y[offset:offset+batch_size]\n",
    "        \n",
    "            yield shuffle(batch_X, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.76500000000000001"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_nn_for_class(X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 12377)\n",
      "(100, 12377)\n",
      "(400, 10)\n",
      "(100, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_family,y_family,test_size=0.2,random_state=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def keras_nn_for_family(X_train, X_test, Y_train, Y_test, dropout = 0.5, batch_size = 32):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Flatten, Dense, Lambda, Cropping2D, Dropout\n",
    "    from keras.layers.convolutional import Convolution2D\n",
    "    \n",
    "    input_node = X_train.shape[1]\n",
    "    layer = []\n",
    "    \n",
    "    \n",
    "    train_generator = generator(X_train, Y_train, batch_size=batch_size)\n",
    "    validation_generator = generator(X_test, Y_test, batch_size=batch_size)\n",
    "    \n",
    "    input_shape =(X_train.shape[1],)  # Trimmed image format\n",
    "    #print(\"input_shape:\", input_shape)\n",
    "        \n",
    "    model = Sequential()\n",
    "    # Preprocess incoming data, centered around zero with small standard deviation \n",
    "    model.add(Dense(input_node, activation='relu', input_shape=input_shape , kernel_initializer='normal'))\n",
    "    layer.append(input_node)\n",
    "    model.add(Dropout(dropout)) # for preventing overfit\n",
    "    layer.append(\"Dropout\")\n",
    "    input_node = input_node//2\n",
    "    \n",
    "    \n",
    "    while(input_node > 10):\n",
    "        model.add(Dense(input_node, activation='relu',  kernel_initializer='normal'))\n",
    "        layer.append(input_node)\n",
    "        model.add(Dropout(dropout)) # for preventing overfit\n",
    "        layer.append(\"Dropout\")\n",
    "        input_node = input_node//2\n",
    "        \n",
    "        \n",
    "    model.add(Dense(10, activation='softmax', kernel_initializer='normal'))\n",
    "    layer.append(10)\n",
    "    #print('layer:', layer)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    #########################################################################\n",
    "    # train a model\n",
    "\n",
    "    history_object = model.fit_generator(train_generator, steps_per_epoch=len(X_train)/batch_size,\n",
    "                        validation_data=validation_generator,\n",
    "                        validation_steps=len(X_test)/batch_size, epochs=10, verbose=0)\n",
    "    \n",
    "    loss, score = model.evaluate_generator(validation_generator, steps=len(X_test)/batch_size, max_queue_size=10, workers=1, use_multiprocessing=True)\n",
    "    return score\n",
    "\n",
    "def generator(X, y, batch_size=32):\n",
    "    from sklearn.utils import shuffle\n",
    "    \n",
    "    num_samples = len(X)\n",
    "    \n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_X = X[offset:offset+batch_size]\n",
    "            batch_y = y[offset:offset+batch_size]\n",
    "        \n",
    "            yield shuffle(batch_X, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.089999999999999997"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_nn_for_family(X_train, X_test, Y_train, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
